{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe import tasks\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/pose_landmarker_full.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pose landmarker instance with the video mode:\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=tasks.BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=vision.RunningMode.VIDEO)\n",
    "\n",
    "def video2landmarks(path: str):\n",
    "    \"\"\"動画からポーズランドマークを取得する\n",
    "\n",
    "    Args:\n",
    "        path (str): 動画ファイルのパス\n",
    "\n",
    "    Yields:\n",
    "        vision.PoseLandmarkerResult: ポーズランドマーク\n",
    "    \"\"\"\n",
    "\n",
    "    with vision.PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise\n",
    "\n",
    "        while True:\n",
    "            ret, cv2_image = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2_image)\n",
    "            timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "            pose_landmarker_result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "            yield pose_landmarker_result\n",
    "\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = Path('videos/pose')\n",
    "videos = list(pose_path.iterdir())\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos[0].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(videos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = str(videos[0])\n",
    "resg = video2landmarks(p)\n",
    "res = list(resg)\n",
    "res[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = res[0]\n",
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = res0.pose_landmarks[0]\n",
    "pls[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pls[0]\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pl.x, pl.y, pl.z, pl.presence, pl.visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwls = res0.pose_world_landmarks[0]\n",
    "pwls[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwl = pwls[0]\n",
    "pwl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pwl.x, pwl.y, pwl.z, pwl.presence, pwl.visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result2np(result: vision.PoseLandmarkerResult):\n",
    "    \"\"\"ランドマークをnumpyデータに変換する\n",
    "\n",
    "    Args:\n",
    "        result (vision.PoseLandmarkerResult): ランドマーク\n",
    "\n",
    "    Returns:\n",
    "        NDArray: numpyデータ\n",
    "    \"\"\"\n",
    "\n",
    "    landmarks = result.pose_landmarks[0]\n",
    "    coords = [(l.x, l.y, l.z) for l in landmarks]\n",
    "    return np.array(coords)\n",
    "\n",
    "def valid_result(result: vision.PoseLandmarkerResult):\n",
    "    \"\"\"ランドマークがnumpyデータに変換できるかどうかを検証する\n",
    "\n",
    "    Args:\n",
    "        result (vision.PoseLandmarkerResult): ランドマーク\n",
    "\n",
    "    Returns:\n",
    "        boolean: 変換可能であればTrue、そうでなければFalse\n",
    "    \"\"\"\n",
    "    return 0 < len(result.pose_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa0 = result2np(res0)\n",
    "npa0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {v.stem: list(video2landmarks(str(v))) for v in videos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_results = min(results_dict.values(), key=lambda v: len(v))\n",
    "len(min_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "samples = [(label, random.choices([r for r in results if valid_result(r)], k=sample_size)) for label, results in result_dict.items()]\n",
    "samples.sort(key=lambda label, : label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label, _ in samples]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(result2np(result), i) for i, (_, results)in enumerate(samples) for result in results]\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([data for data, _ in dataset])\n",
    "y = np.array([label for _, label in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(x, y, train_size=0.8)\n",
    "input_shape = (train_x.shape[1], train_x.shape[2])\n",
    "label_len = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape, name='input'),\n",
    "    keras.layers.Flatten(name='Flatten'),\n",
    "    keras.layers.Dense(64, activation='relu', name='Dense'),\n",
    "    keras.layers.Dropout(0.1, name='Dropout'),\n",
    "    keras.layers.Dense(label_len, activation='softmax', name='Output'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = keras_model.fit(train_x, train_y, epochs=20, validation_data=(val_x, val_y))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size = 100\n",
    "test_samples = [(label, random.choices([r for r in results if valid_result(r)], k=test_sample_size)) for label, results in result_dict.items()]\n",
    "\n",
    "test_dataset = [(result2np(result), i) for i, (_, results)in enumerate(test_samples) for result in results]\n",
    "\n",
    "test_x = np.array([data for data, _ in test_dataset])\n",
    "test_y = np.array([label for _, label in test_dataset])\n",
    "\n",
    "keras_model.evaluate(test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
